program: optimize_jailbreak_attacks_against_vlms.py
project: universal-vlm-jailbreak
method: grid
parameters:
  compile:
    values:
      [False]
  data:
    parameters:
      batch_size:
        values:
          [2]
      dataset:
        values:
          [
            "advbench",
            "rylan_anthropic_hhh",
            "generated",
          ]
      num_workers:
        values:
          [ 4 ]
      prefetch_factor:
        values: [ 4 ]
      split:
        values:
          [ "train" ]
  image_kwargs:
    parameters:
      image_size:
        values:
          [512]
      image_initialization:
        values:
          [
            "random",
            "trina",
          ]
  lightning_kwargs:
    parameters:
      accumulate_grad_batches:
        values:
          [4]
      gradient_clip_val:
        values:
          [10.0]
      limit_train_batches:
        values:
          [1.0]
      log_loss_every_n_steps:
        values:
          [1]
      log_image_every_n_steps:
        values:
          [250]
      precision:
        values:
          ["bf16-mixed"]
  models_to_attack:
    values:
      [
        "{'prism-gemma-instruct+8b+clip', 'prism-llama2-chat+7b+clip', 'prism-llama3-instruct+8b+clip', 'prism-mistral-instruct-v0.2+7b+clip', 'prism-gemma-instruct+8b+siglip', 'prism-llama2-chat+7b+siglip', 'prism-llama3-instruct+8b+siglip', 'prism-mistral-instruct-v0.2+7b+siglip'}"
        "{'prism-gemma-instruct+8b+clip', 'prism-llama2-chat+7b+clip', 'prism-llama3-instruct+8b+clip', 'prism-mistral-instruct-v0.2+7b+clip', 'prism-gemma-instruct+8b+dinosiglip', 'prism-llama2-chat+7b+dinosiglip', 'prism-llama3-instruct+8b+dinosiglip', 'prism-mistral-instruct-v0.2+7b+dinosiglip'}",
        "{'prism-gemma-instruct+2b+clip', 'prism-gemma-instruct+8b+clip', 'prism-llama2-chat+7b+clip', 'prism-llama3-instruct+8b+clip', 'prism-mistral-instruct-v0.2+7b+clip', 'prism-reproduction-llava-v15+7b', 'prism-clip+7b', 'prism-clip+13b'}",
        "{'prism-gemma-instruct+2b+clip', 'prism-gemma-instruct+2b+siglip', 'prism-gemma-instruct+8b+clip', 'prism-gemma-instruct+8b+siglip', 'prism-llama2-chat+7b+clip', 'prism-llama2-chat+7b+siglip', 'prism-llama3-instruct+8b+clip', 'prism-llama3-instruct+8b+siglip'}",
        "{'prism-gemma-instruct+2b+clip', 'prism-gemma-instruct+2b+dinosiglip', 'prism-gemma-instruct+8b+clip', 'prism-gemma-instruct+8b+dinosiglip', 'prism-llama2-chat+7b+clip', 'prism-llama2-chat+7b+dinosiglip', 'prism-llama3-instruct+8b+clip', 'prism-llama3-instruct+8b+dinosiglip'}",
      ]
  n_generations:
    values:
      [ 30 ]
  n_grad_steps:
    values:
      [25000]
  optimization:
    parameters:
      eps:
        values:
          [0.0001]
      learning_rate:
        values:
          [0.001]
      momentum:
        values:
          [0.9]
      optimizer:
        values:
          ["adam"]
      weight_decay:
        values:
          [0.00001]
  seed:
    values:
      [0]